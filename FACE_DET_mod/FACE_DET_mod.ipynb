{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18866a37",
   "metadata": {},
   "source": [
    "### MAPPING the Train and Test Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = '/kaggle/input/manualdataceleb/train'\n",
    "testData = '/kaggle/input/manualdataceleb/test'# paths for traning and testing dataset within Kaggle working directory\n",
    "dnnModel = 'hogModel.h5'# deep learning classifier target model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "lab_list = os.listdir(trainData)\n",
    "print(lab_list) \n",
    "# for getting the list labelling of the dataset; a verification step\n",
    "# this module can be run when inside the Kaggle working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f81627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable declaration\n",
    "imgW = 512\n",
    "imgH = 512\n",
    "\n",
    "# image  pixel depth. Important for input Neuron definitions. \n",
    "# Change the image definitions for processed image to say 128 X 128\n",
    "\n",
    "trainSamples = 200\n",
    "testSamples = 50\n",
    "\n",
    "'''\n",
    "train vs test. This has to be changed according to the image set and \n",
    "training versus testing data bifurcation\n",
    "\n",
    "'''\n",
    "epochs = 10\n",
    "batchSize = 10\n",
    "# signifies the training epoch and batch size. Batch size is increased according to the data size\n",
    "# total size/batch size = runs during model generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e13a8a",
   "metadata": {},
   "source": [
    "Further there will be THREE models - ONLY one is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43765b2",
   "metadata": {},
   "source": [
    "###  Keras DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878548ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FOR DNN model\n",
    "\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape = input_shape))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'rmsprop', \n",
    "                  loss = 'CategoricalCrossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model():\n",
    "        \n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    model = createModel((512,512,3))\n",
    "    print(model)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "            trainData,\n",
    "            target_size = (imgW, imgH),\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False)\n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(\n",
    "            testData,\n",
    "            target_size = (imgW, imgH),\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False)\n",
    "    \n",
    "    model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = trainSamples, \n",
    "            epochs = epochs,\n",
    "            validation_data = test_generator,\n",
    "            validation_steps = trainSamples\n",
    "            )\n",
    "    model.summary()\n",
    "    model.save(dnnModel)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/kaggle/working/finalModel.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812036e9",
   "metadata": {},
   "source": [
    "###  Keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "KERAS CNN\n",
    "\n",
    "'''\n",
    "\n",
    "def createModel(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model():\n",
    "        \n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    model = createModel((512,512,3))\n",
    "    print(model)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "            trainData,\n",
    "            target_size = (imgW, imgH),\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False)\n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(\n",
    "            testData,\n",
    "            target_size = (imgW, imgH),\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False)\n",
    "    \n",
    "    model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = trainSamples, \n",
    "            epochs = epochs,\n",
    "            validation_data = test_generator,\n",
    "            validation_steps = trainSamples\n",
    "            )\n",
    "    model.summary()\n",
    "    model.save(dnnModel)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/kaggle/working/finalModel.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f32cf",
   "metadata": {},
   "source": [
    "### TF Keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TF Keras - CNN\n",
    "\n",
    "'''\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "rescaled = ImageDataGenerator(rescale=1.0 / 255,\n",
    "    rotation_range=5,\n",
    "    zoom_range=(0.95, 0.95),\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    data_format=\"channels_first\",\n",
    "    validation_split=0.0,\n",
    "    dtype=tf.float32)\n",
    "train_fed = tf.keras.utils.image_dataset_from_directory('/kaggle/input/manualdataceleb/train', image_size = (512,512), batch_size=8, label_mode = 'categorical')\n",
    "test_fed = tf.keras.utils.image_dataset_from_directory('/kaggle/input/manualdataceleb/test', image_size = (512,512), batch_size=8, label_mode = 'categorical')\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape=(512,512,3)),\n",
    "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),  #1st layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),  #2nd layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    #tf.keras.layers.MaxPool2D(2,2),  #3rd layer\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    #tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    #tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    #tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),  #4th layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')                                                       \n",
    "])\n",
    "\n",
    "def train_model():\n",
    "        \n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    model = createModel((512,512,3))\n",
    "    print(model)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "            trainData,\n",
    "            target_size = (imgW, imgH),\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False)\n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(\n",
    "            testData,\n",
    "            target_size = (imgW, imgH),\n",
    "            batch_size = batchSize,\n",
    "            shuffle = False)\n",
    "    \n",
    "    model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = trainSamples, \n",
    "            epochs = epochs,\n",
    "            validation_data = test_generator,\n",
    "            validation_steps = trainSamples\n",
    "            )\n",
    "    model.summary()\n",
    "    model.save(dnnModel)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/kaggle/working/finalModel.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d189e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiler for TF Keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TF Keras\n",
    "history = model.fit(train_fed, shuffle=True, epochs=20, validation_data=test_fed, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7357082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b019d451",
   "metadata": {},
   "source": [
    "## IMAGE PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToRGB(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba670b",
   "metadata": {},
   "source": [
    "#### HAAR Cascade and HOG filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc057370",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_face_cascade = cv2.CascadeClassifier('data/haarcascade_frontalface_alt.xml')\n",
    "haar_face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "haar_eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "basepath = '/kaggle/input/facesceleb'\n",
    "\n",
    "pathArr = []\n",
    "fpathArr = []\n",
    "\n",
    "with os.scandir(basepath) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_dir():\n",
    "            #print(entry.name)\n",
    "            folder = Path(entry.path)\n",
    "            files_in_entry = folder.iterdir()\n",
    "            for item in files_in_entry:\n",
    "                if item.is_file():\n",
    "                    #print(\"\\t\"+item.name)\n",
    "                    #print(os.getcwd())\n",
    "                    pathArr.append(entry.path)\n",
    "                    fpathArr.append(entry.path+\"/\"+item.name)\n",
    "\n",
    "#print(pathArr)\n",
    "\n",
    "for i in range(1,len(fpathArr)):\n",
    "    print(fpathArr[i]+\"...\")\n",
    "    fPath = fpathArr[i]\n",
    "    face_img = cv2.imread(fPath)\n",
    "    \n",
    "    \n",
    "\n",
    "    gray_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #plt.imshow(gray_img, cmap='gray')\n",
    "    \n",
    "    #plt.imshow(gray_img, cmap='gray')\n",
    "    \n",
    "    #plt.show()\n",
    "\n",
    "    faces = haar_face_cascade.detectMultiScale(gray_img, scaleFactor=1.1, minNeighbors=1)\n",
    "    \n",
    "    cropped_image = face_img\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if(len(faces) == 1):\n",
    "        print('Face found')\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(face_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        #plt.imshow(convertToRGB(test1))\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "        cropped_image = face_img[y:y+h, x:x+w]\n",
    "\n",
    "        #plt.imshow(convertToRGB(cropped_image))\n",
    "\n",
    "        #plt.show()\n",
    "    \n",
    "        #plt.axis(\"off\")\n",
    "        #plt.imshow(img)\n",
    "        #print(img.shape)\n",
    "\n",
    "        resized_img = resize(cropped_image, (128*4, 64*4))\n",
    "        plt.axis(\"off\")\n",
    "        #plt.imshow(resized_img)\n",
    "        #plt.show()\n",
    "        #print(resized_img.shape)\n",
    "\n",
    "        #creating hog features\n",
    "        fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                \tcells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "        #print(fd.shape)\n",
    "        #print(hog_image.shape)\n",
    "        plt.axis(\"off\")\n",
    "        #plt.imshow(hog_image, cmap=\"gray\")\n",
    "        #plt.show()\n",
    "        \n",
    "        pathArr[i] = pathArr[i].replace(\"input/facesceleb\",\"working/facesceleb\")\n",
    "    \n",
    "        if not os.path.exists(pathArr[i]):\n",
    "            os.makedirs(pathArr[i])\n",
    "    \n",
    "        \n",
    "        fPath = fPath.replace(\"input/facesceleb\",\"working/facesceleb\")\n",
    "        #plt.imsave(\"resized_img.jpg\", resized_img)\n",
    "        plt.imsave(fPath, hog_image, cmap=\"gray\")\n",
    "    else:\n",
    "        #fPath = fPath.replace(\"input/facesceleb\",\"working/facesceleb\")\n",
    "        pathArr[i] = pathArr[i].replace(\"input/facesceleb\",\"working/facenotfound\")\n",
    "        \n",
    "        if not os.path.exists(pathArr[i]):\n",
    "            os.makedirs(pathArr[i])\n",
    "        \n",
    "        fPath = fPath.replace(\"input/facesceleb\",\"working/facenotfound\")\n",
    "        \n",
    "        print('check')\n",
    "        \n",
    "        print(fPath)\n",
    "        \n",
    "        #cv2.imwrite(fPath, face_img)\n",
    "        \n",
    "        #face_img.save(fPath)\n",
    "        \n",
    "        plt.imsave(fPath, face_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "downloading from Kaggle ZIP file \n",
    "Kaggle will not allow folder download \n",
    "\n",
    "'''\n",
    "import shutil\n",
    "shutil.make_archive(\"facenotfound\", 'zip', \"/kaggle/working/facenotfound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59c8d9",
   "metadata": {},
   "source": [
    "After Train Test split Upload the files as Zip file into new folder and follow to the\n",
    "#### MAPPING the Train and Test Folders section \n",
    "\n",
    "and then run the same deep learning model as attempted in the previous step, say example\n",
    "\n",
    "### TF Keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37474648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74eb0e5e",
   "metadata": {},
   "source": [
    "### Code segment For Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c23a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "basepath = '/kaggle/input/facedata/train'\n",
    "with os.scandir(basepath) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_dir():\n",
    "            print(entry.name)\n",
    "            entry = Path(entry.path)\n",
    "            files_in_entry = entry.iterdir()\n",
    "            for item in files_in_entry:\n",
    "                if item.is_file():\n",
    "                    print(\"\\t\"+item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab5707",
   "metadata": {},
   "source": [
    "## Generating Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d5251",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_img = cv2.imread('/kaggle/input/facesceleb/FacesAlexDaddario/A24.jpg')\n",
    "\n",
    "print(\"Original Image\")\n",
    "\n",
    "plt.imshow(convertToRGB(face_img))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"GrayScale Image\")\n",
    "\n",
    "gray_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(gray_img, cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.imshow(gray_img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's detect multiscale images\n",
    "faces = haar_face_cascade.detectMultiScale(gray_img, scaleFactor=1.1, minNeighbors=1)\n",
    "\n",
    "#print the number of faces found\n",
    "if(len(faces)>0):\n",
    "    print('Faces found=',len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5247037",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(face_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "from PIL import Image\n",
    "\n",
    "print(\"Image with Face Rectangle\")\n",
    "\n",
    "plt.imshow(convertToRGB(face_img))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Cropped Face Image\")\n",
    "cropped_image = face_img[y:y+h, x:x+w]\n",
    "\n",
    "plt.imshow(convertToRGB(cropped_image))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#reading the image\n",
    "img = convertToRGB(cropped_image)\n",
    "imshow(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#reading the image\n",
    "#img = imread('/kaggle/input/facedata/test/1026/13432.jpg')\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)\n",
    "print(img.shape)\n",
    "\n",
    "#resizing image\n",
    "print(\"Resized Image\")\n",
    "resized_img = resize(img, (128*4, 64*4))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(resized_img)\n",
    "plt.show()\n",
    "print(resized_img.shape)\n",
    "\n",
    "#creating hog features\n",
    "fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                \tcells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "print(fd.shape)\n",
    "print(hog_image.shape)\n",
    "\n",
    "print(\"HOG Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(hog_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# save the images\n",
    "plt.imsave(\"resized_img.jpg\", resized_img)\n",
    "plt.imsave(\"hog_image.jpg\", hog_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5303d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
